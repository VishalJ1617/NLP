{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3578,
     "status": "ok",
     "timestamp": 1703214979479,
     "user": {
      "displayName": "Vishal Jagadale",
      "userId": "11393755916970364780"
     },
     "user_tz": -330
    },
    "id": "lcMEWH-rF42X",
    "outputId": "ba869117-45b1-466e-b407-4c6bd6f780dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UgSVAfTGhuM"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1703214979480,
     "user": {
      "displayName": "Vishal Jagadale",
      "userId": "11393755916970364780"
     },
     "user_tz": -330
    },
    "id": "dOO2BqbhF5sN"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 560,
     "status": "ok",
     "timestamp": 1703215224481,
     "user": {
      "displayName": "Vishal Jagadale",
      "userId": "11393755916970364780"
     },
     "user_tz": -330
    },
    "id": "gNF5oSYnGdh5"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer):\n",
    "  def __init__(self, embed_dim, num_heads, ff_dim, rate = 0.1,):\n",
    "    # embed_dim: This parameter specifies the dimentionality of the\n",
    "    # num_heas: This parameter controls the number of attentioni head\n",
    "    # ff_dim: This parameter specifies the dimentionality of the feedforward network\n",
    "    # ratw: This parameter controls the dropout rate, which is used to\n",
    "\n",
    "    super().__init__()\n",
    "    self.att = MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)\n",
    "    # this creates a MultiHeadAttention layer, responsoble for learning\n",
    "    self.ffn  = Sequential(\n",
    "        [Dense(ff_dim, activation = \"relu\"), Dense(embed_dim),]\n",
    "    )\n",
    "    # self.ffn: This creates a feedforward network, often used for add\n",
    "    self.layernormal1 = LayerNormalization(epsilon=1e-6)\n",
    "    self.layernormal2 = LayerNormalization(epsilon=1e-6)\n",
    "    # self.layernormal1 and self.layernormal2: These create\n",
    "    self.dropout1 = Dropout(rate)\n",
    "    self.dropout2 = Dropout(rate)\n",
    "    # self.dropout1 and self.dropout2: These create Dropout layers\n",
    "\n",
    "  def call(self, inputs, training) :\n",
    "    attn_output = self.att(inputs, inputs)\n",
    "    # Applies the nulti-head attention to the input sequence, allowing different\n",
    "    attn_output = self.dropout1(attn_output, training = training)\n",
    "    # Applies dropout to the attention output\n",
    "    out1 = self.layernormal1(inputs + attn_output)\n",
    "    # add the attention output to the original input and applies layer\n",
    "    ffn_output = self.ffn(out1)\n",
    "    # Passes the normalized output through the feedforwrfs network\n",
    "    ffn_output = self.dropout2(ffn_output,training = training)\n",
    "\n",
    "    return self.layernormal2(out1+ffn_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1703215224993,
     "user": {
      "displayName": "Vishal Jagadale",
      "userId": "11393755916970364780"
     },
     "user_tz": -330
    },
    "id": "NPXYrwTPNXV7"
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(Layer):\n",
    "  def __init__(self,maxlen, vocab_size, embed_dim):\n",
    "    # maxlen: the maximium length of the input sequence the model will\n",
    "    # vocab size : The total number of unique tokens (words)in the words\n",
    "    # embed_dim : The dimentionality of the embeddings (how each wors is taken according to the postion)\n",
    "    super().__init__()\n",
    "    self.token_emb = Embedding(input_dim = vocab_size, output_dim = embed_dim)\n",
    "    # An embedding layer that maps each token in the input sequence\n",
    "    # to a dense vector of size embed_dim\n",
    "    self.pos_emb = Embedding(input_dim = maxlen, output_dim = embed_dim)\n",
    "    # An embedding layer that maps each position in the sequence\n",
    "    # (from 0 to maxlen - 1) to a dense vector of size embed_dim\n",
    "\n",
    "\n",
    "  def call(self, x):\n",
    "    maxlen = tf.shape(x)[-1]\n",
    "    # Extracts the actual length of the current input sequence\n",
    "    positions = tf.range(start = 0, limit = maxlen, delta = 1)\n",
    "    # create a tensor of position from 0 to maxlen-1\n",
    "    positions = self.pos_emb(positions)\n",
    "    # looks up the position embedding for each position in the sequence\n",
    "    x = self.token_emb(x)\n",
    "    # looks up the token embedding for each position in the sequence\n",
    "    return x + positions\n",
    "    # adds the token embedding and positioni embeding element-wise\n",
    "    # resulting in a combined representation that captures boht word\n",
    "    # meaning and position information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5847,
     "status": "ok",
     "timestamp": 1703215230832,
     "user": {
      "displayName": "Vishal Jagadale",
      "userId": "11393755916970364780"
     },
     "user_tz": -330
    },
    "id": "9-OGkmm9PP4O",
    "outputId": "3ebe76de-13a1-4eb6-aaf1-45fcbd054a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training Sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "vocab_size=20000 #Only consider top 20k words\n",
    "maxlen=200 # Only consider the first 200 words of each movie review\n",
    "(x_train,y_train),(x_val,y_val)=imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train),\"Training Sequences\")\n",
    "print(len(x_val),\"Validation sequences\")\n",
    "x_train=pad_sequences(x_train,maxlen=maxlen)\n",
    "x_val=pad_sequences(x_val,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1703215230832,
     "user": {
      "displayName": "Vishal Jagadale",
      "userId": "11393755916970364780"
     },
     "user_tz": -330
    },
    "id": "LHlIUzW9QwP8",
    "outputId": "5ccd39b4-fdd2-416d-f8c7-bf875a338268"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 200)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1703215231277,
     "user": {
      "displayName": "Vishal Jagadale",
      "userId": "11393755916970364780"
     },
     "user_tz": -330
    },
    "id": "g3QKnn6HQ3d6"
   },
   "outputs": [],
   "source": [
    "embed_dim=32 #Embedding size of each token\n",
    "num_heads=2 #NUMBER OF attention heads\n",
    "ff_dim=32 #Hidden layer size in feedforward network inside transformer\n",
    "\n",
    "inputs=Input(shape=(maxlen,))\n",
    "embedding_layer=TokenAndPositionEmbedding(maxlen,vocab_size,embed_dim)\n",
    "x=embedding_layer(inputs)\n",
    "transformer_block=TransformerBlock(embed_dim,num_heads,ff_dim)\n",
    "x=transformer_block(x)\n",
    "x=GlobalAveragePooling1D()(x)\n",
    "x=Dropout(0.1)(x)\n",
    "x=Dense(20,activation=\"relu\")(x)\n",
    "x=Dropout(0.1)(x)\n",
    "outputs=Dense(2,activation=\"softmax\")(x)\n",
    "\n",
    "model=Model(inputs=inputs,outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1285157,
     "status": "ok",
     "timestamp": 1703216516419,
     "user": {
      "displayName": "Vishal Jagadale",
      "userId": "11393755916970364780"
     },
     "user_tz": -330
    },
    "id": "n-AjzlBzf99Q",
    "outputId": "7f3cf9c9-485c-469a-d159-dee1ec5286f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 129s 162ms/step - loss: 0.3937 - accuracy: 0.8078 - val_loss: 0.3061 - val_accuracy: 0.8670\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 120s 153ms/step - loss: 0.1962 - accuracy: 0.9247 - val_loss: 0.3124 - val_accuracy: 0.8740\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 128s 164ms/step - loss: 0.1297 - accuracy: 0.9550 - val_loss: 0.3915 - val_accuracy: 0.8628\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 126s 161ms/step - loss: 0.0839 - accuracy: 0.9725 - val_loss: 0.5455 - val_accuracy: 0.8303\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 131s 168ms/step - loss: 0.0553 - accuracy: 0.9822 - val_loss: 0.6159 - val_accuracy: 0.8454\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 126s 162ms/step - loss: 0.0396 - accuracy: 0.9879 - val_loss: 0.7414 - val_accuracy: 0.8469\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 115s 147ms/step - loss: 0.0281 - accuracy: 0.9922 - val_loss: 0.8096 - val_accuracy: 0.8422\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 128s 164ms/step - loss: 0.0278 - accuracy: 0.9914 - val_loss: 0.8560 - val_accuracy: 0.8372\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 125s 159ms/step - loss: 0.0238 - accuracy: 0.9927 - val_loss: 0.7702 - val_accuracy: 0.8352\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 124s 158ms/step - loss: 0.0182 - accuracy: 0.9948 - val_loss: 0.8569 - val_accuracy: 0.8344\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "history=model.fit(x_train,y_train,batch_size=32,epochs=10,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1703216594043,
     "user": {
      "displayName": "Vishal Jagadale",
      "userId": "11393755916970364780"
     },
     "user_tz": -330
    },
    "id": "NbajOHqfidX8",
    "outputId": "6e061b3c-7ff3-4a87-e467-591b33fb90a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, 200, 32)           646400    \n",
      " ng_3 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_block_3 (Trans  (None, 200, 32)           10656     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 32)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 20)                660       \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 657758 (2.51 MB)\n",
      "Trainable params: 657758 (2.51 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2rn7XnnnraK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNGudpffgxpsAYPjVaiQSIi",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
