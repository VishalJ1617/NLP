{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d708c98",
   "metadata": {},
   "source": [
    "Date : / /2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725b5b8",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74d365",
   "metadata": {},
   "source": [
    "<img src=\"93033pic1.png\" alt=\"word2vec model\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b21bb09",
   "metadata": {},
   "source": [
    "1. Word2Vec is a group of related models used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2Vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space.\n",
    "\n",
    "2. Word2Vec can utilize either of two model architectures to produce these distributed representations of words: Continuous Bag-Of-Words (CBOW) or continuously sliding skip-gram. In both architectures, Word2Vec considers both individual words and a sliding context window as it iterates over the corpus 2.\n",
    "\n",
    "3. In the CBOW architecture, the model predicts the current word based on the context words. The CBOW can be viewed as a 'fill in the blank' task, where the word embedding represents the way the word influences the relative probabilities of other words in the context window 2.\n",
    "\n",
    "In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words. The skip-gram architecture weighs nearby context words more heavily than more distant context words. According to the authors' note, CBOW is faster while skip-gram does a better job for infrequent words 2.\n",
    "\n",
    "After the model has trained, the learned word embeddings are positioned in the vector space such that words that share common contexts in the corpus — that is, words that are semantically and syntactically similar — are located close to one another in the space. More dissimilar words are located farther from one another in the space 2.\n",
    "\n",
    "Word2Vec was introduced by a team of researchers at Google led by Tomas Mikolov. Google hosts an open-source version of Word2Vec released under an Apache 2.0 license. In 2014, Mikolov left Google for Facebook, and in May 2015, Google was granted a patent for the method, which does not abrogate the Apache license under which it has been released 1.\n",
    "\n",
    "Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through Word2Vec have proven to be successful on a variety of downstream natural language processing tasks 4.\n",
    "\n",
    "The output of the Word2Vec neural net is a vocabulary in which each item has a vector attached to it, which can be fed into a deep-learning net or simply queried to detect relationships between words 1.\n",
    "\n",
    "In terms of implementation, you can use the Keras Subclassing API to define your Word2Vec model. This involves defining layers for target and context embeddings, and a function that computes the dot product of target and context embeddings from a training pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64001fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e561c15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f407208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27dc9b1d",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de893d",
   "metadata": {},
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cf12a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------------\n",
    "### Importing libraries\n",
    "### -------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import nltk, string, gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import json    # Library to work with data json format\n",
    "\n",
    "from textblob import Word\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe0f5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package abc to /home/dai/nltk_data...\n",
      "[nltk_data]   Package abc is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d2ef790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import abc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f60ff",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6cec1f",
   "metadata": {},
   "source": [
    "### Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2bf3b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['PM', 'denies', 'knowledge', 'of', 'AWB', 'kickbacks', 'The', 'Prime', 'Minister', 'has', 'denied', 'he', 'knew', 'AWB', 'was', 'paying', 'kickbacks', 'to', 'Iraq', 'despite', 'writing', 'to', 'the', 'wheat', 'exporter', 'asking', 'to', 'be', 'kept', 'fully', 'informed', 'on', 'Iraq', 'wheat', 'sales', '.'], ['Letters', 'from', 'John', 'Howard', 'and', 'Deputy', 'Prime', 'Minister', 'Mark', 'Vaile', 'to', 'AWB', 'have', 'been', 'released', 'by', 'the', 'Cole', 'inquiry', 'into', 'the', 'oil', 'for', 'food', 'program', '.'], ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ----------\n",
    "### input data\n",
    "### ----------\n",
    "\n",
    "abc.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9bbe667",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(abc.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9fe1584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('law', 0.9344958662986755), ('policy', 0.9130299091339111), ('general', 0.9113556742668152), ('agriculture', 0.9049243927001953), ('education', 0.9046909213066101), ('discussion', 0.9044297933578491), ('media', 0.9019559025764465), ('biology', 0.8990610837936401), ('physics', 0.8984453082084656), ('department', 0.8983114361763)]\n"
     ]
    }
   ],
   "source": [
    "# printing the vector of science (word similar to science)\n",
    "\n",
    "data = model.wv.most_similar('science')\n",
    "\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44439b",
   "metadata": {},
   "source": [
    "### Reading json file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6b7e7",
   "metadata": {},
   "source": [
    "1. json file is in the format of key ---- > value pair.\n",
    "2. The current file that we are reading is : intent.jason\n",
    "3. json file can also be read with pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44bae8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tag': 'welcome',\n",
       "  'patterns': ['Hi',\n",
       "   'How are you',\n",
       "   'Is any one to talk?',\n",
       "   'Hello',\n",
       "   'hi are you available'],\n",
       "  'responses': ['Hello, thanks for contacting us',\n",
       "   'Good to see you here',\n",
       "   ' Hi there, how may I assist you?']},\n",
       " {'tag': 'goodbye',\n",
       "  'patterns': ['Bye', 'See you later', 'Goodbye', 'I will come back soon'],\n",
       "  'responses': ['See you later, thanks for visiting',\n",
       "   'have a great day ahead',\n",
       "   'Wish you Come back again soon.']},\n",
       " {'tag': 'thankful',\n",
       "  'patterns': ['Thanks for helping me',\n",
       "   'Thank your guidance',\n",
       "   \"That's helpful and kind from you\"],\n",
       "  'responses': ['Happy to help!',\n",
       "   'Any time!',\n",
       "   'My pleasure',\n",
       "   'It is my duty to help you']},\n",
       " {'tag': 'hoursopening',\n",
       "  'patterns': ['What hours are you open?',\n",
       "   'Tell your opening time?',\n",
       "   'When are you open?',\n",
       "   'Just your timing please'],\n",
       "  'responses': [\"We're open every day 8am-7pm\",\n",
       "   'Our office hours are 8am-7pm every day',\n",
       "   'We open office at 8 am and close at 7 pm']},\n",
       " {'tag': 'payments',\n",
       "  'patterns': ['Can I pay using credit card?',\n",
       "   ' Can I pay using Mastercard?',\n",
       "   ' Can I pay using cash only?'],\n",
       "  'responses': ['We accept VISA, Mastercard and credit card',\n",
       "   'We accept credit card, debit cards and cash. Please don’t worry']}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file that we are reading is : 'intents.json'\n",
    "# data path : https://mitu.co.in/dataset/\n",
    "\n",
    "json_file = 'intents.json'\n",
    "\n",
    "with open('intents.json', 'r') as fh:\n",
    "    \n",
    "    data = json.load(fh)\n",
    "\n",
    "\n",
    "    \n",
    "# Printing the data\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedfb73f",
   "metadata": {},
   "source": [
    "##### Reading file with pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b360660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>patterns</th>\n",
       "      <th>responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welcome</td>\n",
       "      <td>[Hi, How are you, Is any one to talk?, Hello, ...</td>\n",
       "      <td>[Hello, thanks for contacting us, Good to see ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goodbye</td>\n",
       "      <td>[Bye, See you later, Goodbye, I will come back...</td>\n",
       "      <td>[See you later, thanks for visiting, have a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thankful</td>\n",
       "      <td>[Thanks for helping me, Thank your guidance, T...</td>\n",
       "      <td>[Happy to help!, Any time!, My pleasure, It is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hoursopening</td>\n",
       "      <td>[What hours are you open?, Tell your opening t...</td>\n",
       "      <td>[We're open every day 8am-7pm, Our office hour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>payments</td>\n",
       "      <td>[Can I pay using credit card?,  Can I pay usin...</td>\n",
       "      <td>[We accept VISA, Mastercard and credit card, W...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tag                                           patterns  \\\n",
       "0       welcome  [Hi, How are you, Is any one to talk?, Hello, ...   \n",
       "1       goodbye  [Bye, See you later, Goodbye, I will come back...   \n",
       "2      thankful  [Thanks for helping me, Thank your guidance, T...   \n",
       "3  hoursopening  [What hours are you open?, Tell your opening t...   \n",
       "4      payments  [Can I pay using credit card?,  Can I pay usin...   \n",
       "\n",
       "                                           responses  \n",
       "0  [Hello, thanks for contacting us, Good to see ...  \n",
       "1  [See you later, thanks for visiting, have a gr...  \n",
       "2  [Happy to help!, Any time!, My pleasure, It is...  \n",
       "3  [We're open every day 8am-7pm, Our office hour...  \n",
       "4  [We accept VISA, Mastercard and credit card, W...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "\n",
    "df = pd.read_json('intents.json')\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e19906a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>patterns</th>\n",
       "      <th>responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welcome</td>\n",
       "      <td>Hi, How are you, Is any one to talk?, Hello, h...</td>\n",
       "      <td>[Hello, thanks for contacting us, Good to see ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goodbye</td>\n",
       "      <td>Bye, See you later, Goodbye, I will come back ...</td>\n",
       "      <td>[See you later, thanks for visiting, have a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thankful</td>\n",
       "      <td>Thanks for helping me, Thank your guidance, Th...</td>\n",
       "      <td>[Happy to help!, Any time!, My pleasure, It is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hoursopening</td>\n",
       "      <td>What hours are you open?, Tell your opening ti...</td>\n",
       "      <td>[We're open every day 8am-7pm, Our office hour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>payments</td>\n",
       "      <td>Can I pay using credit card?,  Can I pay using...</td>\n",
       "      <td>[We accept VISA, Mastercard and credit card, W...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tag                                           patterns  \\\n",
       "0       welcome  Hi, How are you, Is any one to talk?, Hello, h...   \n",
       "1       goodbye  Bye, See you later, Goodbye, I will come back ...   \n",
       "2      thankful  Thanks for helping me, Thank your guidance, Th...   \n",
       "3  hoursopening  What hours are you open?, Tell your opening ti...   \n",
       "4      payments  Can I pay using credit card?,  Can I pay using...   \n",
       "\n",
       "                                           responses  \n",
       "0  [Hello, thanks for contacting us, Good to see ...  \n",
       "1  [See you later, thanks for visiting, have a gr...  \n",
       "2  [Happy to help!, Any time!, My pleasure, It is...  \n",
       "3  [We're open every day 8am-7pm, Our office hour...  \n",
       "4  [We accept VISA, Mastercard and credit card, W...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the list of pattern into string\n",
    "\n",
    "df['patterns'] = df['patterns'].apply(', '.join)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e8065c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245457c",
   "metadata": {},
   "source": [
    "**what does following command do:**\n",
    "\n",
    "**df['patterns'] = df['patterns'].str.replace('[^\\w\\s]','')**\n",
    "\n",
    "Answer | Phind V9 Model\n",
    "\n",
    "The command df['patterns'] = df['patterns'].str.replace('[^\\w\\s]','') is used in Python's pandas library to replace all non-alphanumeric characters in the 'patterns' column of the dataframe df with an empty string.\n",
    "\n",
    "Let's break it down:\n",
    "\n",
    "    df['patterns']: This is selecting the 'patterns' column from the dataframe df.\n",
    "\n",
    "    .str.replace('[^\\w\\s]',''): This is using the str.replace() function, which is a pandas function used to replace occurrences of a pattern/regex in the Series/Index. The function is being called on the 'patterns' column. The argument '[^\\w\\s]' is a regular expression that matches any character that is not a word character (denoted by \\w) or a whitespace character (denoted by \\s). The second argument '' is the replacement string, which in this case is an empty string. This means that any character that matches the pattern [^\\w\\s] will be replaced with nothing, effectively removing it.\n",
    "\n",
    "So, in summary, this command is removing all non-alphanumeric characters from the 'patterns' column of the dataframe df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de627f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['patterns'] = df['patterns'].apply(lambda x:' '.join(x.lower() for x in x.split()))\n",
    "\n",
    "df['patterns'] = df['patterns'].apply(lambda x:' '.join(x for x in word_tokenize(x) if x not in string.punctuation))\n",
    "\n",
    "df['patterns'] = df['patterns'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "df['patterns'] = df['patterns'].apply(lambda x: ' '.join(x for x in x.split() if not x.isdigit()))\n",
    "\n",
    "df['patterns'] = df['patterns'].apply(lambda x: ' '.join(x for x in x.split() if not x in stop))\n",
    "\n",
    "df['patterns'] = df['patterns'].apply(lambda x: ' '. join([Word(word).lemmatize() for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d00891e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       hi one talk hello hi available\n",
       "1                 bye see later goodbye come back soon\n",
       "2        thanks helping thank guidance 's helpful kind\n",
       "3       hour open tell opening time open timing please\n",
       "4    pay using credit card pay using mastercard pay...\n",
       "Name: patterns, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['patterns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2676226b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hi', 'one', 'talk', 'hello', 'hi', 'available'],\n",
       " ['bye', 'see', 'later', 'goodbye', 'come', 'back', 'soon'],\n",
       " ['thanks', 'helping', 'thank', 'guidance', \"'s\", 'helpful', 'kind'],\n",
       " ['hour', 'open', 'tell', 'opening', 'time', 'open', 'timing', 'please'],\n",
       " ['pay',\n",
       "  'using',\n",
       "  'credit',\n",
       "  'card',\n",
       "  'pay',\n",
       "  'using',\n",
       "  'mastercard',\n",
       "  'pay',\n",
       "  'using',\n",
       "  'cash']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking out the outer list\n",
    "\n",
    "bigger_list = []\n",
    "\n",
    "for i in df['patterns']:\n",
    "    \n",
    "    li = i.split()\n",
    "    \n",
    "    bigger_list.append(li)\n",
    "\n",
    "\n",
    "bigger_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bce80a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=32, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# custom data is fed to machine for further processing\n",
    "\n",
    "model = Word2Vec(bigger_list, min_count = 1, workers = 4)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e7c2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model with .save method\n",
    "\n",
    "model.save('Word2Vec.model')\n",
    "\n",
    "model.save('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1479fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Word2Vec.load('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c25678aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['using',\n",
       " 'pay',\n",
       " 'hi',\n",
       " 'open',\n",
       " 'later',\n",
       " 'soon',\n",
       " 'back',\n",
       " 'come',\n",
       " 'goodbye',\n",
       " 'bye',\n",
       " 'see',\n",
       " 'helping',\n",
       " 'available',\n",
       " 'hello',\n",
       " 'talk',\n",
       " 'one',\n",
       " 'thanks',\n",
       " 'cash',\n",
       " 'thank',\n",
       " 'mastercard',\n",
       " \"'s\",\n",
       " 'helpful',\n",
       " 'kind',\n",
       " 'hour',\n",
       " 'tell',\n",
       " 'opening',\n",
       " 'time',\n",
       " 'timing',\n",
       " 'please',\n",
       " 'credit',\n",
       " 'card',\n",
       " 'guidance']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(new_model.wv.key_to_index)\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71406645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('one', 0.19613029062747955), ('pay', 0.18879325687885284), ('guidance', 0.14262470602989197), ('credit', 0.13661333918571472), ('hour', 0.10765139758586884), ('see', 0.09932278841733932), ('bye', 0.07770184427499771), ('opening', 0.0754380002617836), ('helpful', 0.06751954555511475), ('mastercard', 0.04943053424358368)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = new_model.wv.most_similar('kind')\n",
    "\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be12b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
