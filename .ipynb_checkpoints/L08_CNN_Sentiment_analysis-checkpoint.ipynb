{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d775b6e9",
   "metadata": {},
   "source": [
    "Date : 16/12/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985d240c",
   "metadata": {},
   "source": [
    "## CNN for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c906f",
   "metadata": {},
   "source": [
    "<img src=\"\" alt=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0c928d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/dai/anaconda3/lib/python3.11/site-packages (3.2.4)\n",
      "Requirement already satisfied: six in /home/dai/anaconda3/lib/python3.11/site-packages (from nltk) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78692f1",
   "metadata": {},
   "source": [
    "!pip install keras_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e67962e",
   "metadata": {},
   "source": [
    "!pip install keras preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad95c175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/dai/anaconda3/lib/python3.11/site-packages (3.2.4)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /home/dai/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/dai/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/dai/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/dai/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Installing collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.2.4\n",
      "    Uninstalling nltk-3.2.4:\n",
      "      Successfully uninstalled nltk-3.2.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip  install nltk --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02049d39",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'acyclic_depth_first' from 'nltk.util' (/home/dai/anaconda3/lib/python3.11/site-packages/nltk/util.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m acyclic_depth_first \u001b[38;5;28;01mas\u001b[39;00m acyclic_tree\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'acyclic_depth_first' from 'nltk.util' (/home/dai/anaconda3/lib/python3.11/site-packages/nltk/util.py)"
     ]
    }
   ],
   "source": [
    "from nltk.util import acyclic_depth_first as acyclic_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47ad05ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'acyclic_depth_first' from 'nltk.util' (/home/dai/anaconda3/lib/python3.11/site-packages/nltk/util.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/__init__.py:153\u001b[0m\n\u001b[1;32m    149\u001b[0m toolbox \u001b[38;5;241m=\u001b[39m lazyimport\u001b[38;5;241m.\u001b[39mLazyModule(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnltk.toolbox\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Optional loading\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/translate/__init__.py:24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleu_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentence_bleu \u001b[38;5;28;01mas\u001b[39;00m bleu\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mribes_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentence_ribes \u001b[38;5;28;01mas\u001b[39;00m ribes\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeteor_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m meteor_score \u001b[38;5;28;01mas\u001b[39;00m meteor\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alignment_error_rate\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstack_decoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StackDecoder\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/translate/meteor_score.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chain, product\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Iterable, List, Tuple\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetCorpusReader, wordnet\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StemmerI\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/corpus/__init__.py:64\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mNLTK corpus readers.  The modules in this package provide functions\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mthat can be used to read corpus files in a variety of formats.  These\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyCorpusLoader\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RegexpTokenizer\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/corpus/reader/__init__.py:81\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbnc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnps_chat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwordnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mswitchboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:351\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpertainyms\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    348\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_related(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 351\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSynset\u001b[39;00m(_WordNetObject):\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Synset from a \"<lemma>.<pos>.<number>\" string where:\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    <lemma> is the word's morphological stem\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m    <pos> is one of the module attributes ADJ, ADJ_SAT, ADV, NOUN or VERB\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    - pertainyms\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;18m__slots__\u001b[39m \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_offset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_min_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    418\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:608\u001b[0m, in \u001b[0;36mSynset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m synset \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    606\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m synset\n\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m acyclic_depth_first \u001b[38;5;28;01mas\u001b[39;00m acyclic_tree\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unweighted_minimum_spanning_tree \u001b[38;5;28;01mas\u001b[39;00m mst\n\u001b[1;32m    611\u001b[0m \u001b[38;5;66;03m# Also add this shortcut?\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;66;03m#    from nltk.util import unweighted_minimum_spanning_digraph as umsd\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'acyclic_depth_first' from 'nltk.util' (/home/dai/anaconda3/lib/python3.11/site-packages/nltk/util.py)"
     ]
    }
   ],
   "source": [
    "### --------------------\n",
    "### Importing Librarires\n",
    "### --------------------\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Embedding, Conv1D, MaxPooling1D\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40fbd180",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d64e483",
   "metadata": {},
   "source": [
    "### Defining the function to read files one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "\n",
    "def load_doc(filename):\n",
    "    \n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    \n",
    "    #read all text\n",
    "    text = file.read()\n",
    "    \n",
    "    # close the file\n",
    "    file.close()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c885f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tetsting the 'load_doc' function\n",
    "\n",
    "# reading a random file from the directory\n",
    "\n",
    "text = load_doc('review_polarity/txt_sentoken/pos/cv026_29325.txt')\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d05d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn a doc into clean token\n",
    "def clean_doc(doc):\n",
    "    #Split ino tokens by white space\n",
    "    tokens=doc.split()\n",
    "    #Prepare regax for char filtering\n",
    "    re_punc=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    #remove punctuation from each words\n",
    "    tokens=[re_punc.sub('',w)for w in tokens]\n",
    "    #remove remaining tokens that are not alphabetic\n",
    "    tokens=[word for word in tokens if word.isalpha()]\n",
    "    #filter out stop words\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    tokens=[w for w in tokens if not w in stop_words]\n",
    "    #Filter out short tokens\n",
    "    tokens=[word for word in tokens if len(word)>1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5239ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean=clean_doc(text)\n",
    "len(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944d802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename,vocab):\n",
    "    #load the doc\n",
    "    doc=load_doc(filename)\n",
    "    #clean doc\n",
    "    tokens=clean_doc(doc)\n",
    "    #Filter the vocab\n",
    "    tokens=[w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ab60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=open('vocab.txt')\n",
    "vocab=vocab.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231a910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_to_line('review_polarity/txt_sentoken/pos/cv026_29325.txt',vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f46c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seprating traing and test data\n",
    "def process_train(directory,vocab):\n",
    "    documents=list()\n",
    "     # to check files in directory\n",
    "    for filename in listdir(directory):\n",
    "        if not filename.startswith('cv9'):\n",
    "            path=directory + '/'+filename\n",
    "            doc=load_doc(path)\n",
    "            tokens=clean_doc(doc)\n",
    "            documents.append(tokens)\n",
    "    return documents\n",
    "    \n",
    "    \n",
    "# Test data\n",
    "def process_test(directory,vocab):\n",
    "    documents=list()\n",
    "     # to check files in directory\n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            path=directory + '/'+filename\n",
    "            doc=load_doc(path)\n",
    "            tokens=clean_doc(doc)\n",
    "            documents.append(tokens)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c68c85",
   "metadata": {},
   "source": [
    "### Function to read all the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory,vocab,is_train):\n",
    "    documents = list()\n",
    "    #walks through all the files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        #skip any reviews in test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        #create a full path of the file to open\n",
    "        path = directory + '/'+filename\n",
    "        #load the doc\n",
    "        doc = load_doc(path)\n",
    "        #clean the data\n",
    "        tokens = clean_doc(doc)\n",
    "        #load the data\n",
    "        documents.append(tokens)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b91a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = process_docs('review_polarity/review_polarity/txt_sentoken/pos', vocab, False)\n",
    "\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean a dataset\n",
    "\n",
    "\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    \n",
    "    # load documents\n",
    "    neg = process_docs('review_polarity/review_polarity/txt_sentoken/neg',\n",
    "                      vocab, is_train)\n",
    "    \n",
    "    pos = process_docs('review_polarity/review_polarity/txt_sentoken/pos',\n",
    "                      vocab, is_train)\n",
    "    \n",
    "    docs = neg + pos\n",
    "    \n",
    "    # prepare labels\n",
    "    \n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    \n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8245f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels = load_clean_dataset(vocab, True)\n",
    "test, test_labels = load_clean_dataset(vocab, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d89465",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deeff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test), len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae86b9a",
   "metadata": {},
   "source": [
    "## Build ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "\n",
    "\n",
    "\n",
    "def define_model(n_words):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # define network\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50,\n",
    "                   input_shape = (n_words,),\n",
    "                   activation = 'relu'\n",
    "                   )\n",
    "             )\n",
    "    \n",
    "    model.add(Dense(1,\n",
    "                   activation = 'sigmoid'\n",
    "                   )\n",
    "             )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # compile network\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                 optimizer = 'adam',\n",
    "                 metrics = ['accuracy']\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # summarize the defined model\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plotting the model\n",
    "    \n",
    "    plot_model(model, \n",
    "              to_file = 'model.png',\n",
    "              show_shapes = True\n",
    "              )\n",
    "\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model call\n",
    "\n",
    "define_model(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1531fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    \n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Create the tokenizer\n",
    "\n",
    "tokenizer = create_tokenizer(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3986c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode and pad documents\n",
    "\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    \n",
    "    #integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    \n",
    "    # pad sequence\n",
    "    padded = pad_sequences(encoded, maxlen = max_length, padding = 'post')\n",
    "    \n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbac126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode data\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(train,\n",
    "                                   mode = 'binary')\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ecf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tokenizer.texts_to_matrix(test,\n",
    "                                   mode = 'binary')\n",
    "\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e220e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e2b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define network\n",
    "\n",
    "n_words=x_train.shape[1]\n",
    "\n",
    "model=define_model(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181cf749",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,np.array(train_labels),batch_size=10,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd52d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the architecture of the model\n",
    "\n",
    "plot_model(model, \n",
    "           show_dtype = True,\n",
    "          show_layer_activations = True,\n",
    "          show_shapes = True,\n",
    "          show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0815e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, np.array(test_labels), batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4dfc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "text11 = 'Best movie ever! It was great, I will definitely recommend it.'\n",
    "text22 = 'This is a bad movie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3c1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review):\n",
    "    \n",
    "    #clean\n",
    "    token = clean_doc(review)\n",
    "    \n",
    "    #convert to line\n",
    "    line = ' '.join(token)\n",
    "    \n",
    "    #encode\n",
    "    encoded = tokenizer.texts_to_matrix((line), mode = 'binary')\n",
    "    \n",
    "    #predict sentiment\n",
    "    yhat = model.predict(encoded, verbose = 0)\n",
    "    \n",
    "    # retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    \n",
    "    if round(percent_pos) == 0:\n",
    "        \n",
    "        return (1 - percent_pos), 'NEGATIVE'\n",
    "    \n",
    "    return percent_pos, 'POSITIVE'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8037d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage, sentiment=predict_sentiment(text22)\n",
    "print(percentage,sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9514b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(text11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897953c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
