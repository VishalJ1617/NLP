{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b0e62ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89770311",
   "metadata": {},
   "source": [
    "### Golbal variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471c704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/home/dai/7. NLP & CV/NLP/fruits'\n",
    "trainDir = '/training_set/' \n",
    "testDir = '/test_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cdbb584",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrainDir = filepath + trainDir\n",
    "\n",
    "dataTestDir = filepath + testDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaaec698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orange', 'apple']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dataTestDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d70c5a6",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c602e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1/255,\n",
    "                                  shear_range=0.2,\n",
    "                                  zoom_range=0.2,\n",
    "                                  horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949d0ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75013bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method flow_from_directory in module keras.src.preprocessing.image:\n",
      "\n",
      "flow_from_directory(directory, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', follow_links=False, subset=None, interpolation='nearest', keep_aspect_ratio=False) method of keras.src.preprocessing.image.ImageDataGenerator instance\n",
      "    Takes the path to a directory & generates batches of augmented data.\n",
      "    \n",
      "    Args:\n",
      "      directory: string, path to the target directory. It should contain\n",
      "        one subdirectory per class. Any PNG, JPG, BMP, PPM or TIF images\n",
      "        inside each of the subdirectories directory tree will be included\n",
      "        in the generator. See [this script](\n",
      "        https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d)\n",
      "        for more details.\n",
      "      target_size: Tuple of integers `(height, width)`. The dimensions to\n",
      "        which all images found will be resized. Defaults to `(256,256)`.\n",
      "      color_mode: One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n",
      "        Whether the images will be converted to have 1, 3, or 4 channels.\n",
      "      classes: Optional list of class subdirectories (e.g. `['dogs',\n",
      "        'cats']`). Default: None. If not provided, the list of classes\n",
      "        will be automatically inferred from the subdirectory\n",
      "        names/structure under `directory`, where each subdirectory will be\n",
      "        treated as a different class (and the order of the classes, which\n",
      "        will map to the label indices, will be alphanumeric). The\n",
      "        dictionary containing the mapping from class names to class\n",
      "        indices can be obtained via the attribute `class_indices`.\n",
      "      class_mode: One of \"categorical\", \"binary\", \"sparse\",\n",
      "        \"input\", or None.\n",
      "        Determines the type of label arrays that are returned:\n",
      "          - \"categorical\" will be 2D one-hot encoded labels,\n",
      "          - \"binary\" will be 1D binary labels,\n",
      "          - \"sparse\" will be 1D integer labels,\n",
      "          - \"input\" will be images identical\n",
      "            to input images (mainly used to work with autoencoders).\n",
      "          - If None, no labels are returned\n",
      "            (the generator will only yield batches of image data,\n",
      "            which is useful to use with `model.predict_generator()`).\n",
      "            Please note that in case of class_mode None,\n",
      "            the data still needs to reside in a subdirectory\n",
      "            of `directory` for it to work correctly.\n",
      "          Defaults to \"categorical\".\n",
      "      batch_size: Size of the batches of data. Defaults to `32`.\n",
      "      shuffle: Whether to shuffle the data If `False`, sorts the\n",
      "        data in alphanumeric order. Defaults to `True`.\n",
      "      seed: Optional random seed for shuffling and transformations.\n",
      "      save_to_dir: None or str (default: None). This allows you to\n",
      "        optionally specify a directory to which to save the augmented\n",
      "        pictures being generated (useful for visualizing what you are\n",
      "        doing).\n",
      "      save_prefix: Str. Prefix to use for filenames of saved pictures\n",
      "        (only relevant if `save_to_dir` is set).\n",
      "      save_format: one of \"png\", \"jpeg\", \"bmp\", \"pdf\", \"ppm\", \"gif\",\n",
      "        \"tif\", \"jpg\" (only relevant if `save_to_dir` is set).\n",
      "        Defaults to \"png\".\n",
      "      follow_links: Whether to follow symlinks inside\n",
      "        class subdirectories. Defaults to `False`.\n",
      "      subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
      "        `validation_split` is set in `ImageDataGenerator`.\n",
      "      interpolation: Interpolation method used to resample the image if\n",
      "        the target size is different from that of the loaded image.\n",
      "        Supported methods are `\"nearest\"`, `\"bilinear\"`, and `\"bicubic\"`.\n",
      "        If PIL version 1.1.3 or newer is installed, `\"lanczos\"` is also\n",
      "        supported. If PIL version 3.4.0 or newer is installed, `\"box\"` and\n",
      "        `\"hamming\"` are also supported. Defaults to `\"nearest\"`.\n",
      "      keep_aspect_ratio: Boolean, whether to resize images to a target\n",
      "        size without aspect ratio distortion. The image is cropped in\n",
      "        the center with target aspect ratio before resizing.\n",
      "    \n",
      "    Returns:\n",
      "      A `DirectoryIterator` yielding tuples of `(x, y)`\n",
      "        where `x` is a numpy array containing a batch\n",
      "        of images with shape `(batch_size, *target_size, channels)`\n",
      "        and `y` is a numpy array of corresponding labels.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_datagen.flow_from_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ea55c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = train_datagen.flow_from_directory(dataTrainDir, \n",
    "                                         class_mode = 'binary',\n",
    "                                         target_size = (64,64)\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "241eaeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test = train_datagen.flow_from_directory(dataTestDir, \n",
    "                                         class_mode = 'binary',\n",
    "                                         target_size = (64,64)\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bb1c4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.preprocessing.image.ImageDataGenerator at 0x7f4518134110>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.image_data_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12fe514",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6d22399",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3,3),\n",
    "                input_shape = (64,64,3), activation = 'relu')\n",
    "         )\n",
    "\n",
    "model.add(MaxPool2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 8, kernel_size = (3,3),\n",
    "                 activation = 'relu')\n",
    "         )\n",
    "\n",
    "model.add(MaxPool2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93faeb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 16)        448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 31, 31, 16)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 8)         1160      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 14, 14, 8)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1568)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                25104     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26729 (104.41 KB)\n",
      "Trainable params: 26729 (104.41 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### -------------\n",
    "### Model outline\n",
    "### -------------\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7151c",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6b1221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -----------------\n",
    "### Compile the model\n",
    "### -----------------\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7057fcc",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abea3a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6884 - accuracy: 0.6923 - val_loss: 0.7081 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.6737 - accuracy: 0.5385 - val_loss: 0.6995 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.6733 - accuracy: 0.5385 - val_loss: 0.6889 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.6401 - accuracy: 0.5385 - val_loss: 0.6437 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.6255 - accuracy: 0.7692 - val_loss: 0.6130 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.5891 - accuracy: 1.0000 - val_loss: 0.5646 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.5625 - accuracy: 1.0000 - val_loss: 0.5390 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.5191 - accuracy: 1.0000 - val_loss: 0.5060 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.4683 - accuracy: 1.0000 - val_loss: 0.4550 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.4241 - accuracy: 1.0000 - val_loss: 0.4181 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f4518121b10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, epochs = 10, validation_data = test,\n",
    "         batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5788876f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': 0, 'orange': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = train.class_indices\n",
    "\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a16e5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ImageDataGenerator(rescale = 1/255,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "803a5ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = plt.imread('/home/dai/7. NLP & CV/NLP/fruits/sample1.jpg')\n",
    "sample2 = plt.imread('/home/dai/7. NLP & CV/NLP/fruits/sample2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed4e7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = cv2.resize(sample1, (64,64))\n",
    "sample1 = sample1.reshape(1,64,64,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ba65d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = cv2.resize(sample2, (64,64))\n",
    "sample2 = sample1.reshape(1,64,64,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f36cc2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "orange\n"
     ]
    }
   ],
   "source": [
    "sample1_predict = model.predict(sample1, batch_size=1)[0][0]\n",
    "\n",
    "for key, value in classes.items():\n",
    "    \n",
    "    if value == int(sample1_predict):\n",
    "        \n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27bf91d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "orange\n"
     ]
    }
   ],
   "source": [
    "sample2_predict = model.predict(sample2, batch_size=1)[0][0]\n",
    "\n",
    "for key, value in classes.items():\n",
    "    \n",
    "    if value == int(sample2_predict):\n",
    "        \n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e17a404",
   "metadata": {},
   "source": [
    "### Another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18f81717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "image is of : orange\n"
     ]
    }
   ],
   "source": [
    "test_image = load_img('/home/dai/7. NLP & CV/NLP/fruits/sample1.jpg', \n",
    "                     target_size = (64,64))\n",
    "\n",
    "test_image = img_to_array(test_image)\n",
    "\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "test_image.shape\n",
    "\n",
    "sample_1_pred = model.predict(test_image, batch_size = 1)[0][0]\n",
    "\n",
    "for key, value in classes.items():\n",
    "    \n",
    "    if value == int(sample_1_pred):\n",
    "        \n",
    "        print(f'image is of : {key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09769d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "image is of : apple\n"
     ]
    }
   ],
   "source": [
    "test2_image = load_img('/home/dai/7. NLP & CV/NLP/fruits/sample2.jpg', \n",
    "                     target_size = (64,64))\n",
    "\n",
    "test2_image = img_to_array(test2_image)\n",
    "\n",
    "test2_image = np.expand_dims(test2_image, axis = 0)\n",
    "\n",
    "test2_image.shape\n",
    "\n",
    "sample_2_pred = model.predict(test2_image, batch_size = 1)[0][0]\n",
    "\n",
    "for key, value in classes.items():\n",
    "    \n",
    "    if value == int(sample_2_pred):\n",
    "        \n",
    "        print(f'image is of : {key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a6dd91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
