{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d775b6e9",
   "metadata": {},
   "source": [
    "Date : 15/12/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985d240c",
   "metadata": {},
   "source": [
    "## ANN for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c906f",
   "metadata": {},
   "source": [
    "<img src=\"\" alt=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47ad05ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-22 08:08:39.572659: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-22 08:08:39.575924: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-22 08:08:39.626468: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-22 08:08:39.626506: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-22 08:08:39.626528: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-22 08:08:39.635542: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-22 08:08:39.636474: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-22 08:08:40.973398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "### --------------------\n",
    "### Importing Librarires\n",
    "### --------------------\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40fbd180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 104]\n",
      "[nltk_data]     Connection reset by peer>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d64e483",
   "metadata": {},
   "source": [
    "### Defining the function to read files one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "\n",
    "def load_doc(filename):\n",
    "    \n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    \n",
    "    #read all text\n",
    "    text = file.read()\n",
    "    \n",
    "    # close the file\n",
    "    file.close()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c885f857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for those of us who weren\\'t yet born when the 1960\\'s rock \\'n\\' rolled around , monterey pop affords an affectionate glimpse of the music that influenced our parents to be hippies . \\nfrom otis redding to jimi hendrix , janis joplin to the mamas and the papas , and jefferson airplane to the who , this documentary is jam-packed with contagious energy . \\nbut i give fair warning that i will reveal the ending , which does not do the rest of the film the justice it deserves . \\nshot in 1969 at an outdoor concert that precluded woodstock , the film defies the stereotype of the general population at the time . \\nsure , some have painted their faces and smoke joints , but d . a . \\npennebaker ( the war room , moon over broadway ) surprisingly chooses to show a broad spectrum of the audience . \\nno matter who is watching , it all comes back to the talented musicians that stir your soul . \\nthe excitement starts before the music even begins . \\na young girl is cleaning thousands of seats and when asked why by an interviewer , she replies that she feels lucky to do so . \\nthere are moments of organized craziness as john phillips , leader of the mamas and the papas and one of the concert organizers , tries to get in touch with dionne warwick . \\nand when one band is tuning up , a member remarks , \" finally , a decent sound system ! \" \\nyou can tell just by watching these first few moments that this show isn\\'t about vanity , it\\'s about playing the music you love to those who have an appreciation for it , a two-way street . \\nthis interaction between audience and performer continues throughout the film and becomes infectious to the audience . \\nit\\'s impossible to tear your eyes away from janis joplin as she belts out her ballad about love being a ball and chain . \\nand while the lyrics to \" wild thing \" may not be all that complicated , watching jimi hendrix mime having sex with his guitar is as captivating as otis redding singing about love . \\neven if you don\\'t recognize every band you see on stage , you can imagine being as enthralled by their work as the public sitting in those seats . \\nthe only drawback to the film is the ending , which unfortunately i must reveal . \\nall the other bands , big names then and still today , got approximately 7 to 10 minutes of screen time . \\nin contrast , the last band on camera , a wholly forgettable one , gets an entire 18 minutes of screen time . \\nfor a film that\\'s only 78 minutes long , that\\'s too large of a chunk , especially when previous acts are much more stimulating . \\nall in all , monterey pop is a precious , rare look at a time period that still holds sway over us . \\nthe variety of music , as well as the beautifully shot performances , are easy to become immersed in . \\nif there was ever any question as to why most of these bands were so popular , this is quickly dispelled . \\nit\\'s almost depressing to think that music this moving doesn\\'t get made much anymore . \\ninstead we\\'re stuck with * nsync , the backstreet boys , and jennifer lopez , all of whom should have stuck with modeling . \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tetsting the 'load_doc' function\n",
    "\n",
    "# reading a random file from the directory\n",
    "\n",
    "text = load_doc('/home/dai/7. NLP & CV/NLP/review_polarity/review_polarity/txt_sentoken/pos/cv026_29325.txt')\n",
    "# /home/dai/7. NLP & CV/NLP/review_polarity/review_polarity/txt_sentoken/pos/cv026_29325.txt\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66d05d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn a doc into clean token\n",
    "def clean_doc(doc):\n",
    "    #Split ino tokens by white space\n",
    "    tokens=doc.split()\n",
    "    #Prepare regax for char filtering\n",
    "    re_punc=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    #remove punctuation from each words\n",
    "    tokens=[re_punc.sub('',w)for w in tokens]\n",
    "    #remove remaining tokens that are not alphabetic\n",
    "    tokens=[word for word in tokens if word.isalpha()]\n",
    "    #filter out stop words\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    tokens=[w for w in tokens if not w in stop_words]\n",
    "    #Filter out short tokens\n",
    "    tokens=[word for word in tokens if len(word)>1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5239ece5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean=clean_doc(text)\n",
    "len(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "944d802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename,vocab):\n",
    "    #load the doc\n",
    "    doc=load_doc(filename)\n",
    "    #clean doc\n",
    "    tokens=clean_doc(doc)\n",
    "    #Filter the vocab\n",
    "    tokens=[w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "940ab60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=open('/home/dai/7. NLP & CV/NLP/vocab.txt')\n",
    "vocab=vocab.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "231a910a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'review_polarity/txt_sentoken/pos/cv026_29325.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/dai/7. NLP & CV/NLP/Flask/L07_CNN_Sentiment_analysis.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dai/7.%20NLP%20%26%20CV/NLP/Flask/L07_CNN_Sentiment_analysis.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m doc_to_line(\u001b[39m'\u001b[39m\u001b[39mreview_polarity/txt_sentoken/pos/cv026_29325.txt\u001b[39m\u001b[39m'\u001b[39m,vocab)\n",
      "\u001b[1;32m/home/dai/7. NLP & CV/NLP/Flask/L07_CNN_Sentiment_analysis.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dai/7.%20NLP%20%26%20CV/NLP/Flask/L07_CNN_Sentiment_analysis.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdoc_to_line\u001b[39m(filename,vocab):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dai/7.%20NLP%20%26%20CV/NLP/Flask/L07_CNN_Sentiment_analysis.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m#load the doc\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dai/7.%20NLP%20%26%20CV/NLP/Flask/L07_CNN_Sentiment_analysis.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     doc\u001b[39m=\u001b[39mload_doc(filename)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dai/7.%20NLP%20%26%20CV/NLP/Flask/L07_CNN_Sentiment_analysis.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m#clean doc\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dai/7.%20NLP%20%26%20CV/NLP/Flask/L07_CNN_Sentiment_analysis.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     tokens\u001b[39m=\u001b[39mclean_doc(doc)\n",
      "\u001b[1;32m/home/dai/7. NLP & CV/NLP/Flask/L07_CNN_Sentiment_analysis.ipynb Cell 13\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dai/7.%20NLP%20%26%20CV/NLP/Flask/L07_CNN_Sentiment_analysis.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_doc\u001b[39m(filename):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dai/7.%20NLP%20%26%20CV/NLP/Flask/L07_CNN_Sentiment_analysis.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dai/7.%20NLP%20%26%20CV/NLP/Flask/L07_CNN_Sentiment_analysis.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# open the file as read only\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dai/7.%20NLP%20%26%20CV/NLP/Flask/L07_CNN_Sentiment_analysis.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dai/7.%20NLP%20%26%20CV/NLP/Flask/L07_CNN_Sentiment_analysis.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m#read all text\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dai/7.%20NLP%20%26%20CV/NLP/Flask/L07_CNN_Sentiment_analysis.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     text \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'review_polarity/txt_sentoken/pos/cv026_29325.txt'"
     ]
    }
   ],
   "source": [
    "doc_to_line('review_polarity/txt_sentoken/pos/cv026_29325.txt',vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f46c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seprating traing and test data\n",
    "def process_train(directory,vocab):\n",
    "    documents=list()\n",
    "     # to check files in directory\n",
    "    for filename in listdir(directory):\n",
    "        if not filename.startswith('cv9'):\n",
    "            path=directory + '/'+filename\n",
    "            doc=load_doc(path)\n",
    "            tokens=clean_doc(doc)\n",
    "            documents.append(tokens)\n",
    "    return documents\n",
    "    \n",
    "    \n",
    "# Test data\n",
    "def process_test(directory,vocab):\n",
    "    documents=list()\n",
    "     # to check files in directory\n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            path=directory + '/'+filename\n",
    "            doc=load_doc(path)\n",
    "            tokens=clean_doc(doc)\n",
    "            documents.append(tokens)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c68c85",
   "metadata": {},
   "source": [
    "### Function to read all the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76ac359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory,vocab,is_train):\n",
    "    documents = list()\n",
    "    #walks through all the files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        #skip any reviews in test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        #create a full path of the file to open\n",
    "        path = directory + '/'+filename\n",
    "        #load the doc\n",
    "        doc = load_doc(path)\n",
    "        #clean the data\n",
    "        tokens = clean_doc(doc)\n",
    "        #load the data\n",
    "        documents.append(tokens)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13b91a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = process_docs('/home/dai/7. NLP & CV/NLP/review_polarity/review_polarity/txt_sentoken/pos', vocab, False)\n",
    "\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e15f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean a dataset\n",
    "\n",
    "\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    \n",
    "    # load documents\n",
    "    neg = process_docs('/home/dai/7. NLP & CV/NLP/review_polarity/review_polarity/txt_sentoken/neg',\n",
    "                      vocab, is_train)\n",
    "    \n",
    "    pos = process_docs('/home/dai/7. NLP & CV/NLP/review_polarity/review_polarity/txt_sentoken/pos',\n",
    "                      vocab, is_train)\n",
    "    \n",
    "    docs = neg + pos\n",
    "    \n",
    "    # prepare labels\n",
    "    \n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    \n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8245f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels = load_clean_dataset(vocab, True)\n",
    "test, test_labels = load_clean_dataset(vocab, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85d89465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 1800)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3deeff8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test), len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae86b9a",
   "metadata": {},
   "source": [
    "## Build ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d95e0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "\n",
    "\n",
    "\n",
    "def define_model(n_words):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # define network\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50,\n",
    "                   input_shape = (n_words,),\n",
    "                   activation = 'relu'\n",
    "                   )\n",
    "             )\n",
    "    \n",
    "    model.add(Dense(1,\n",
    "                   activation = 'sigmoid'\n",
    "                   )\n",
    "             )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # compile network\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                 optimizer = 'adam',\n",
    "                 metrics = ['accuracy']\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # summarize the defined model\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plotting the model\n",
    "    \n",
    "    plot_model(model, \n",
    "              to_file = 'model.png',\n",
    "              show_shapes = True\n",
    "              )\n",
    "\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7aef54a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5101 (19.93 KB)\n",
      "Trainable params: 5101 (19.93 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.sequential.Sequential at 0x7f98b7de3510>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model call\n",
    "\n",
    "define_model(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1531fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    \n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Create the tokenizer\n",
    "\n",
    "tokenizer = create_tokenizer(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c9a93",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# encode data\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(train,\n",
    "                                   mode = 'binary')\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ecf4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "x_test = tokenizer.texts_to_matrix(test,\n",
    "                                   mode = 'binary')\n",
    "\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e220e35d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "tokenizer.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e2b730",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#define network\n",
    "\n",
    "n_words=x_train.shape[1]\n",
    "\n",
    "model=define_model(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181cf749",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model.fit(x_train,np.array(train_labels),batch_size=10,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd52d0f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Plotting the architecture of the model\n",
    "\n",
    "plot_model(model, \n",
    "           show_dtype = True,\n",
    "          show_layer_activations = True,\n",
    "          show_shapes = True,\n",
    "          show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0815e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model.evaluate(x_test, np.array(test_labels), batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4dfc85",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "text11 = 'Best movie ever! It was great, I will definitely recommend it.'\n",
    "text22 = 'This is a bad movie'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77729cea",
   "metadata": {},
   "source": [
    "def predict_sentiment(review):\n",
    "    \n",
    "    #clean\n",
    "    token = clean_doc(review)\n",
    "    \n",
    "    #convert to line\n",
    "    line = ' '.join(token)\n",
    "    \n",
    "    #encode\n",
    "    encoded = tokenizer.texts_to_matrix((line), mode = 'binary')\n",
    "    \n",
    "    #predict sentiment\n",
    "    yhat = model.predict(encoded, verbose = 0)\n",
    "    \n",
    "    # retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    \n",
    "    if round(percent_pos) == 0:\n",
    "        \n",
    "        return (1 - percent_pos), 'NEGATIVE'\n",
    "    \n",
    "    return percent_pos, 'POSITIVE'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d3485",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def predict_data(text,vocab):\n",
    "    \n",
    "    tokens = clean_doc(text)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    x_new = tokenizer.texts_to_matrix([tokens],mode = 'binary')\n",
    "    y_pred = model.predict(x_new,verbose=0)\n",
    "    if round(y_pred[0][0])==1:\n",
    "        print(\"Positive Review: \", y_pred[0][0])\n",
    "    elif(round(y_pred[0][0])==0):\n",
    "        print(\"Negative Review: \", y_pred[0][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8037d12a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "predict_data(text22,vocab)\n",
    "print(percentage,sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9514b1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "predict_data(text11, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897953c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
